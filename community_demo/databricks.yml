# These is the default bundle configuration if not otherwise overridden in
# the "targets" top-level mapping.
bundle: # Required.
  name: community_demo # Required.

variables:
    env:
      description: Name of the env where you are doing a deployment

# These are the permissions to apply to experiments, jobs, models, and pipelines defined # in the "resources" mapping.
permissions:
  - level: CAN_MANAGE
    service_principal_name: "fba6beb7-2b89-46dd-95a5-f30505b8ffc2"

# These are the targets to use for deployments and workflow runs. One and only one of these
# targets can be set to "default: true".
targets:
  dev:
    default: true
    variables:
      env: dev
    mode: development
    workspace:
      host: https://adb-2453982529917037.17.azuredatabricks.net/
    run_as:
      service_principal_name: "fba6beb7-2b89-46dd-95a5-f30505b8ffc2"  
  prod:
    variables:
      env: main
    mode: production
    workspace:
      host: https://adb-2960535845402106.6.azuredatabricks.net/
    run_as:
      service_principal_name: "fba6beb7-2b89-46dd-95a5-f30505b8ffc2"

# These are the default job and pipeline settings if not otherwise overridden in
# the following "targets" top-level mapping.
resources:
  jobs:
    Databricks_Storage_Demo:
      name: Databricks Storage Mount and Operations
      tasks:
        - task_key: Mount_Storage
          notebook_task:
            notebook_path: ./Mount_Storage.py
            base_parameters:
              storage_account_name: stgcommdemo
              container_name: demo1
              mount_point: /mnt/demo
              env: ${var.env}
            source: WORKSPACE
          job_cluster_key: Storage_Demo_Cluster

        - task_key: Write_Data
          depends_on:
            - task_key: Mount_Storage
          notebook_task:
            notebook_path: ./Write_Data.py
            base_parameters:
              mount_point: /mnt/demo
              file_name: demo_file.csv
            source: WORKSPACE
          job_cluster_key: Storage_Demo_Cluster

        - task_key: Read_Data
          depends_on:
            - task_key: Write_Data
          notebook_task:
            notebook_path: ./Read_Data.py
            base_parameters:
              mount_point: /mnt/demo
              file_name: demo_file.csv
            source: WORKSPACE
          job_cluster_key: Storage_Demo_Cluster

      job_clusters:
        - job_cluster_key: Storage_Demo_Cluster
          new_cluster:
            spark_version: 14.3.x-scala2.12
            node_type_id: Standard_D4ds_v5
            autoscale:
              min_workers: 1
              max_workers: 4
            spark_env_vars:
              PYSPARK_PYTHON: /databricks/python3/bin/python3
            enable_elastic_disk: true
            runtime_engine: STANDARD
      queue:
        enabled: true